# 🧠正则化方法

虽然不（或少）存旧数据，但是在训练新任务时，往损失函数里加一项（或几项）约束，让模型别把旧任务学到的关键参数/输出分布破坏掉。


## 🔎保护参数（Parameter Importance Regularization）

**核心思想：**

> 重要参数尽量不被更新。

当模型学习新任务时，通过约束参数更新幅度，避免破坏对旧任务至关重要的权重。

**代表方法：**

- **EWC（Elastic Weight Consolidation）**
- **SI（Synaptic Intelligence）**

**基本逻辑：**

- 计算每个参数对旧任务的重要性  
- 在新任务训练时，对重要参数添加惩罚项  
- 限制其偏离原始值  

---

## 🔎保护输出 / 函数行为（Functional Regularization）

**核心思想：**

> 新模型的输出不要偏离旧模型的输出。

即使参数改变，只要函数行为不变，就不会遗忘。

本质上属于一种 **知识蒸馏（Knowledge Distillation）** 思想。

**代表方法：**

- **LwF（Learning without Forgetting）**

**基本逻辑：**

- 保存旧模型的输出（soft targets）  
- 在新任务训练时，约束当前模型输出接近旧模型输出  
- 保持函数层面的稳定性  
