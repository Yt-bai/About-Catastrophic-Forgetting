# 参数隔离

参数不共享，固定主干参数完全冻结 & 针对每个任务训练特定增量参数。每个任务用自己的一套参数，不改旧任务参数，从结构上避免遗忘。
Adapter-based Continual Learning

## 4️⃣ Adapter-based Continual Learning（Adapter 增量学习）


Adapter-based 方法是当前在 Transformer / ViT 等大型模型中非常主流的一类持续学习方案。

---

### 核心思路

整体思路是：

> 主干网络尽量保持稳定（通常冻结或仅做小幅更新），仅为每个新任务新增一组轻量级模块（Adapter）。

---

### 训练机制

- 每层插入一个 Adapter（例如 Transformer 的 12 层各插一个）  
- 训练任务 \( t \) 时，仅更新：
  - 该任务对应的 Adapter
  - 以及可能的分类头（Classifier）
- 旧任务的 Adapter 保持冻结，不被修改
  
---

<img width="770" height="344" alt="image" src="https://github.com/user-attachments/assets/9eeb19e7-e234-4025-b491-67835f5ee945" />

